<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>R on StatLab Articles</title>
    <link>/categories/r/</link>
    <description>Recent content in R on StatLab Articles</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 04 May 2019 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/categories/r/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Simulating a Logistic Regression Model</title>
      <link>/2019/05/04/simulating-a-logistic-regression-model/</link>
      <pubDate>Sat, 04 May 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/05/04/simulating-a-logistic-regression-model/</guid>
      <description>Logistic regression is a method for modeling binary data as a function of other variables. For example we might want to model the occurrence or non-occurrence of a disease given predictors such as age, race, weight, etc. The result is a model that returns a predicted probability of occurrence (or non-occurrence, depending on how we set up our data) given certain values of our predictors. We might also be able to interpret the coefficients in our model to summarize how a change in one predictor affects the odds of occurrence.</description>
    </item>
    
    <item>
      <title>An Introduction to Analyzing Twitter Data with R</title>
      <link>/2019/05/03/an-introduction-to-analyzing-twitter-data-with-r/</link>
      <pubDate>Fri, 03 May 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/05/03/an-introduction-to-analyzing-twitter-data-with-r/</guid>
      <description>In this article, I will walk you through why a researcher or professional might find data from Twitter useful, explain how to collect the relevant tweets and information from Twitter in R, and then finish by demonstrating a few useful analyses (along with accompanying cleaning) you might perform on your Twitter data.
Part One: Why Twitter Data?To begin, we should ask: why would someone be interested in using data from Twitter?</description>
    </item>
    
    <item>
      <title>Getting Started with Multiple Imputation in R</title>
      <link>/2019/05/01/getting-started-with-multiple-imputation-in-r/</link>
      <pubDate>Wed, 01 May 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/05/01/getting-started-with-multiple-imputation-in-r/</guid>
      <description>Whenever we are dealing with a dataset, we almost always run into a problem that may decrease our confidence in the results that we are getting - missing data! Examples of missing data can be found in surveys - where respondents intentionally refrained from answering a question, didn’t answer a question because it is not applicable to them, or simply forgot to give an answer. Or our dataset on trade in agricultural products for country-pairs over years could suffer from missing data as some countries fail to report their accounts for certain years.</description>
    </item>
    
    <item>
      <title>Assessing Type S and Type M Errors</title>
      <link>/2018/10/31/assessing-type-s-and-type-m-errors/</link>
      <pubDate>Wed, 31 Oct 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/10/31/assessing-type-s-and-type-m-errors/</guid>
      <description>The paper Beyond Power Calculations: Assessing Type S (Sign) and Type M (Magnitude) Errors by Andrew Gelman and John Carlin introduces the idea of performing design calculations to help prevent researchers from being misled by statistically significant results in studies with small samples and/or noisy measurements. The main idea is that researchers often overestimate effect sizes in power calculations and collect noisy (ie, highly variable) data, which can make statistically significant results suspect.</description>
    </item>
    
    <item>
      <title>Interpreting Log Transformations in a Linear Model</title>
      <link>/2018/08/17/interpreting-log-transformations-in-a-linear-model/</link>
      <pubDate>Fri, 17 Aug 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/08/17/interpreting-log-transformations-in-a-linear-model/</guid>
      <description>Log transformations are often recommended for skewed data, such as monetary measures or certain biological and demographic measures. Log transforming data usually has the effect of spreading out clumps of data and bringing together spread-out data. For example, below is a histogram of the areas of all 50 US states. It is skewed to the right due to Alaska, California, Texas and a few others.
hist(state.area)After a log transformation, notice the histogram is more or less symmetric.</description>
    </item>
    
    <item>
      <title>Getting Started with Matching Methods</title>
      <link>/2018/04/24/getting-started-with-matching-methods/</link>
      <pubDate>Tue, 24 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/04/24/getting-started-with-matching-methods/</guid>
      <description>A frequent research question is whether or not some “treatment” causes an effect. For example, does taking aspirin daily reduce the chance of a heart attack? Does more sleep lead to better academic performance for teenagers? Does smoking increase the risk of chronic obstructive pulmonary disease (COPD)?
To truly answer such questions, we need a time machine and a lack of ethics. We would need to be able to take a subject and, say, make him or her smoke for several years and observe whether or not they get COPD.</description>
    </item>
    
    <item>
      <title>Getting Started with Moderated Mediation</title>
      <link>/2018/03/02/getting-started-with-moderated-mediation/</link>
      <pubDate>Fri, 02 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/03/02/getting-started-with-moderated-mediation/</guid>
      <description>In a previous post we demonstrated how to perform a basic mediation analysis. In this post we look at performing a moderated mediation analysis.
The basic idea is that a mediator may depend on another variable called a “moderator”. For example, in our mediation analysis post we hypothesized that self-esteem was a mediator of student grades on the effect of student happiness. We illustrate this below with a path diagram.</description>
    </item>
    
    <item>
      <title>Getting started with Multivariate Multiple Regression</title>
      <link>/2017/10/27/getting-started-with-multivariate-multiple-regression/</link>
      <pubDate>Fri, 27 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/10/27/getting-started-with-multivariate-multiple-regression/</guid>
      <description>Multivariate Multiple Regression is the method of modeling multiple responses, or dependent variables, with a single set of predictor variables. For example, we might want to model both math and reading SAT scores as a function of gender, race, parent income, and so forth. This allows us to evaluate the relationship of, say, gender with each score. You may be thinking, “why not just run separate regressions for each dependent variable?</description>
    </item>
    
    <item>
      <title>Visualizing the Effects of Proportional-Odds Logistic Regression</title>
      <link>/2017/05/10/visualizing-the-effects-of-proportional-odds-logistic-regression/</link>
      <pubDate>Wed, 10 May 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/05/10/visualizing-the-effects-of-proportional-odds-logistic-regression/</guid>
      <description>Proportional-odds logistic regression is often used to model an ordered categorical response. By “ordered”, we mean categories that have a natural ordering, such as “Disagree”, “Neutral”, “Agree”, or “Everyday”, “Some days”, “Rarely”, “Never”. For a primer on proportional-odds logistic regression, see our post, Fitting and Interpreting a Proportional Odds Model. In this post we demonstrate how to visualize a proportional-odds model in R.
To begin, we load the effects package.</description>
    </item>
    
    <item>
      <title>Getting started with the purrr package in R</title>
      <link>/2017/04/14/getting-started-with-the-purrr-package-in-r/</link>
      <pubDate>Fri, 14 Apr 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/04/14/getting-started-with-the-purrr-package-in-r/</guid>
      <description>If you’re wondering what exactly the purrr package does, then this blog post is for you.
Before we get started, we should mention the Iteration chapter in R for Data Science by Garrett Grolemund and Hadley Wickham. We think this is the most thorough and extensive introduction to the purrr package currently available (at least at the time of this writing.) Wickham is one of the authors of the purrr package and he spends a good deal of the chapter clearly explaining how it works.</description>
    </item>
    
    <item>
      <title>Working with dates and time in R using the lubridate package</title>
      <link>/2017/01/11/working-with-dates-and-time-in-r-using-the-lubridate-package/</link>
      <pubDate>Wed, 11 Jan 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/01/11/working-with-dates-and-time-in-r-using-the-lubridate-package/</guid>
      <description>Sometimes we have data with dates and/or times that we want to manipulate or summarize. A common example in the health sciences is time-in-study. A subject may enter a study on Feb 12, 2008 and exit on November 4, 2009. How many days was the person in the study? (Don’t forget 2008 was a leap year; February had 29 days.) What was the median time-in-study for all subjects?
Another example are experiments that time participants performing an activity, applies a treatment to certain members, and then re-times the activity.</description>
    </item>
    
    <item>
      <title>The Wilcoxon Rank Sum Test</title>
      <link>/2017/01/05/the-wilcoxon-rank-sum-test/</link>
      <pubDate>Thu, 05 Jan 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/01/05/the-wilcoxon-rank-sum-test/</guid>
      <description>The Wilcoxon Rank Sum Test is often described as the non-parametric version of the two-sample t-test. You sometimes see it in analysis flowcharts after a question such as “is your data normal?” A “no” branch off this question will recommend a Wilcoxon test if you’re comparing two groups of continuous measures.
So what is this Wilcoxon test? What makes it non-parametric? What does that even mean? And how do we implement it and interpret it?</description>
    </item>
    
    <item>
      <title>Pairwise comparisons of proportions</title>
      <link>/2016/10/20/pairwise-comparisons-of-proportions/</link>
      <pubDate>Thu, 20 Oct 2016 00:00:00 +0000</pubDate>
      
      <guid>/2016/10/20/pairwise-comparisons-of-proportions/</guid>
      <description>Pairwise comparison means comparing all pairs of something. If I have three items A, B and C, that means comparing A to B, A to C, and B to C. Given n items, I can determine the number of possible pairs using the binomial coefficient:
\[ \frac{n!}{2!(n - 2)!} = \binom {n}{2}\]
Using the R statistical computing environment, we can use the choose function to quickly calculate this. For example, how many possible 2-item combinations can I “choose” from 10 items:</description>
    </item>
    
    <item>
      <title>A tidyr Tutorial</title>
      <link>/2016/08/24/a-tidyr-tutorial/</link>
      <pubDate>Wed, 24 Aug 2016 00:00:00 +0000</pubDate>
      
      <guid>/2016/08/24/a-tidyr-tutorial/</guid>
      <description>The tidyr package by Hadley Wickham centers on two functions: gather and spread. If you have struggled to understand what exactly these two functions do, this tutorial is for you.
To begin we need to wrap our heads around the idea of “key-value pairs”. The help pages for gather and spread use this terminology to explain how they work. Without some intuition for key-value pairs, it can be difficult to truly understand how these functions work.</description>
    </item>
    
    <item>
      <title>Getting Started with Factor Analysis</title>
      <link>/2016/08/01/getting-started-with-factor-analysis/</link>
      <pubDate>Mon, 01 Aug 2016 00:00:00 +0000</pubDate>
      
      <guid>/2016/08/01/getting-started-with-factor-analysis/</guid>
      <description>Take a look at the following correlation matrix for Olympic decathlon data calculated from 280 scores from 1960 through 2004 (Johnson and Wichern, p. 499):
If we focus on the first row, 100m (100 meter dash), we see that it has fairly high correlations with LJ and 400m (long jump and 400 meter dash, 0.64 and 0.55) and somewhat lower correlations with all other events. Scoring high in the 100 meter dash seems to correlate with scoring high in the long jump and 400 meter run but gives no solid indication of performance in the other events.</description>
    </item>
    
  </channel>
</rss>