<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>StatLab Articles</title>
    <link>/</link>
    <description>Recent content on StatLab Articles</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 14 May 2019 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Reading PDF files into R for text mining</title>
      <link>/2019/05/14/reading-pdf-files-into-r-for-text-mining/</link>
      <pubDate>Tue, 14 May 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/05/14/reading-pdf-files-into-r-for-text-mining/</guid>
      <description>Let’s say we’re interested in text mining the opinions of The Supreme Court of the United States from the 2014 term. The opinions are published as PDF files at the following web page http://www.supremecourt.gov/opinions/slipopinion/14. We would probably want to look at all 76 opinions, but for the purposes of this introductory tutorial we’ll just look at the last three of the term: (1) Glossip v. Gross, (2) State Legislature v.</description>
    </item>
    
    <item>
      <title>Simulating a Logistic Regression Model</title>
      <link>/2019/05/04/simulating-a-logistic-regression-model/</link>
      <pubDate>Sat, 04 May 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/05/04/simulating-a-logistic-regression-model/</guid>
      <description>Logistic regression is a method for modeling binary data as a function of other variables. For example we might want to model the occurrence or non-occurrence of a disease given predictors such as age, race, weight, etc. The result is a model that returns a predicted probability of occurrence (or non-occurrence, depending on how we set up our data) given certain values of our predictors. We might also be able to interpret the coefficients in our model to summarize how a change in one predictor affects the odds of occurrence.</description>
    </item>
    
    <item>
      <title>An Introduction to Analyzing Twitter Data with R</title>
      <link>/2019/05/03/an-introduction-to-analyzing-twitter-data-with-r/</link>
      <pubDate>Fri, 03 May 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/05/03/an-introduction-to-analyzing-twitter-data-with-r/</guid>
      <description>In this article, I will walk you through why a researcher or professional might find data from Twitter useful, explain how to collect the relevant tweets and information from Twitter in R, and then finish by demonstrating a few useful analyses (along with accompanying cleaning) you might perform on your Twitter data.
Part One: Why Twitter Data? To begin, we should ask: why would someone be interested in using data from Twitter?</description>
    </item>
    
    <item>
      <title>Getting Started with Multiple Imputation in R</title>
      <link>/2019/05/01/getting-started-with-multiple-imputation-in-r/</link>
      <pubDate>Wed, 01 May 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/05/01/getting-started-with-multiple-imputation-in-r/</guid>
      <description>Whenever we are dealing with a dataset, we almost always run into a problem that may decrease our confidence in the results that we are getting - missing data! Examples of missing data can be found in surveys - where respondents intentionally refrained from answering a question, didn’t answer a question because it is not applicable to them, or simply forgot to give an answer. Or our dataset on trade in agricultural products for country-pairs over years could suffer from missing data as some countries fail to report their accounts for certain years.</description>
    </item>
    
    <item>
      <title>About</title>
      <link>/about/</link>
      <pubDate>Wed, 17 Apr 2019 21:48:51 -0700</pubDate>
      
      <guid>/about/</guid>
      <description>The UVA StatLab publishes short articles and tutorials on various statistical topics. We try our best to provide clear and accessible explanations. If you have questions, comments or requests, we want to hear from you: statlab@virginia.edu</description>
    </item>
    
    <item>
      <title>Analysis of Ours to Shape Comments, Part 3</title>
      <link>/2018/12/18/analysis-of-ours-to-shape-comments-part-3/</link>
      <pubDate>Tue, 18 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/12/18/analysis-of-ours-to-shape-comments-part-3/</guid>
      <description>Introduction To recap, we’re exploring the comments submitted to President Ryan’s Ours to Shape website (as of December 7, 2018).
In the first post we looked at the distribution of comments across Ryan’s three categories – community, discovery, and service – and across the contributors’ primary connection to the university. We extracted features like length and readability of the comments, and compared these across groups. And we explored the context in which key words of interest (to me) were used.</description>
    </item>
    
    <item>
      <title>Analysis of Ours to Shape Comments, Part 2</title>
      <link>/2018/12/14/analysis-of-ours-to-shape-comments-part-2/</link>
      <pubDate>Fri, 14 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/12/14/analysis-of-ours-to-shape-comments-part-2/</guid>
      <description>Introduction In the last post, we began exploring the Ours to Shape comments – the distribution across categories and contributors, the length and readability of the comments, and a few key words in context. While I did more exploration of the data than reported, the first post gives a taste of the kind of dive into the data that usefully proceeds analysis.
In this post, we’ll start digging into word frequencies, relative frequencies by groups, and distinguishing words.</description>
    </item>
    
    <item>
      <title>Analysis of Ours to Shape Comments, Part 1</title>
      <link>/2018/12/13/analysis-of-ours-to-shape-comments-part-1/</link>
      <pubDate>Thu, 13 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/12/13/analysis-of-ours-to-shape-comments-part-1/</guid>
      <description>Introduction As part of a series of workshops on quantitative analysis of text this fall, I started examining the comments submitted to President Ryan’s Ours to Shape website. The site invites people to share their ideas and insights for UVA going forward, particularly in the domains of service, discovery, and community. The website was only one venue for providing suggestions and voicing possibilities – President Ryan has hosted community discussions as well – but the website afforded an opportunity for individuals to chime in multiple times and at their convenience, so in theory should represent an especially inclusive collection.</description>
    </item>
    
    <item>
      <title>Assessing Type S and Type M Errors</title>
      <link>/2018/10/31/assessing-type-s-and-type-m-errors/</link>
      <pubDate>Wed, 31 Oct 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/10/31/assessing-type-s-and-type-m-errors/</guid>
      <description>The paper Beyond Power Calculations: Assessing Type S (Sign) and Type M (Magnitude) Errors by Andrew Gelman and John Carlin introduces the idea of performing design calculations to help prevent researchers from being misled by statistically significant results in studies with small samples and/or noisy measurements. The main idea is that researchers often overestimate effect sizes in power calculations and collect noisy (ie, highly variable) data, which can make statistically significant results suspect.</description>
    </item>
    
    <item>
      <title>Interpreting Log Transformations in a Linear Model</title>
      <link>/2018/08/17/interpreting-log-transformations-in-a-linear-model/</link>
      <pubDate>Fri, 17 Aug 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/08/17/interpreting-log-transformations-in-a-linear-model/</guid>
      <description>Log transformations are often recommended for skewed data, such as monetary measures or certain biological and demographic measures. Log transforming data usually has the effect of spreading out clumps of data and bringing together spread-out data. For example, below is a histogram of the areas of all 50 US states. It is skewed to the right due to Alaska, California, Texas and a few others.
hist(state.area) After a log transformation, notice the histogram is more or less symmetric.</description>
    </item>
    
    <item>
      <title>Getting Started with Matching Methods</title>
      <link>/2018/04/24/getting-started-with-matching-methods/</link>
      <pubDate>Tue, 24 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/04/24/getting-started-with-matching-methods/</guid>
      <description>A frequent research question is whether or not some “treatment” causes an effect. For example, does taking aspirin daily reduce the chance of a heart attack? Does more sleep lead to better academic performance for teenagers? Does smoking increase the risk of chronic obstructive pulmonary disease (COPD)?
To truly answer such questions, we need a time machine and a lack of ethics. We would need to be able to take a subject and, say, make him or her smoke for several years and observe whether or not they get COPD.</description>
    </item>
    
    <item>
      <title>Getting Started with Moderated Mediation</title>
      <link>/2018/03/02/getting-started-with-moderated-mediation/</link>
      <pubDate>Fri, 02 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/03/02/getting-started-with-moderated-mediation/</guid>
      <description>In a previous post we demonstrated how to perform a basic mediation analysis. In this post we look at performing a moderated mediation analysis.
The basic idea is that a mediator may depend on another variable called a “moderator”. For example, in our mediation analysis post we hypothesized that self-esteem was a mediator of student grades on the effect of student happiness. We illustrate this below with a path diagram.</description>
    </item>
    
    <item>
      <title>Getting started with Multivariate Multiple Regression</title>
      <link>/2017/10/27/getting-started-with-multivariate-multiple-regression/</link>
      <pubDate>Fri, 27 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/10/27/getting-started-with-multivariate-multiple-regression/</guid>
      <description>Multivariate Multiple Regression is the method of modeling multiple responses, or dependent variables, with a single set of predictor variables. For example, we might want to model both math and reading SAT scores as a function of gender, race, parent income, and so forth. This allows us to evaluate the relationship of, say, gender with each score. You may be thinking, “why not just run separate regressions for each dependent variable?</description>
    </item>
    
    <item>
      <title>Visualizing the Effects of Proportional-Odds Logistic Regression</title>
      <link>/2017/05/10/visualizing-the-effects-of-proportional-odds-logistic-regression/</link>
      <pubDate>Wed, 10 May 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/05/10/visualizing-the-effects-of-proportional-odds-logistic-regression/</guid>
      <description>Proportional-odds logistic regression is often used to model an ordered categorical response. By “ordered”, we mean categories that have a natural ordering, such as “Disagree”, “Neutral”, “Agree”, or “Everyday”, “Some days”, “Rarely”, “Never”. For a primer on proportional-odds logistic regression, see our post, Fitting and Interpreting a Proportional Odds Model. In this post we demonstrate how to visualize a proportional-odds model in R.
To begin, we load the effects package.</description>
    </item>
    
    <item>
      <title>Getting started with the purrr package in R</title>
      <link>/2017/04/14/getting-started-with-the-purrr-package-in-r/</link>
      <pubDate>Fri, 14 Apr 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/04/14/getting-started-with-the-purrr-package-in-r/</guid>
      <description>If you’re wondering what exactly the purrr package does, then this blog post is for you.
Before we get started, we should mention the Iteration chapter in R for Data Science by Garrett Grolemund and Hadley Wickham. We think this is the most thorough and extensive introduction to the purrr package currently available (at least at the time of this writing.) Wickham is one of the authors of the purrr package and he spends a good deal of the chapter clearly explaining how it works.</description>
    </item>
    
    <item>
      <title>Working with dates and time in R using the lubridate package</title>
      <link>/2017/01/11/working-with-dates-and-time-in-r-using-the-lubridate-package/</link>
      <pubDate>Wed, 11 Jan 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/01/11/working-with-dates-and-time-in-r-using-the-lubridate-package/</guid>
      <description>Sometimes we have data with dates and/or times that we want to manipulate or summarize. A common example in the health sciences is time-in-study. A subject may enter a study on Feb 12, 2008 and exit on November 4, 2009. How many days was the person in the study? (Don’t forget 2008 was a leap year; February had 29 days.) What was the median time-in-study for all subjects?
Another example are experiments that time participants performing an activity, applies a treatment to certain members, and then re-times the activity.</description>
    </item>
    
    <item>
      <title>The Wilcoxon Rank Sum Test</title>
      <link>/2017/01/05/the-wilcoxon-rank-sum-test/</link>
      <pubDate>Thu, 05 Jan 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/01/05/the-wilcoxon-rank-sum-test/</guid>
      <description>The Wilcoxon Rank Sum Test is often described as the non-parametric version of the two-sample t-test. You sometimes see it in analysis flowcharts after a question such as “is your data normal?” A “no” branch off this question will recommend a Wilcoxon test if you’re comparing two groups of continuous measures.
So what is this Wilcoxon test? What makes it non-parametric? What does that even mean? And how do we implement it and interpret it?</description>
    </item>
    
    <item>
      <title>Pairwise comparisons of proportions</title>
      <link>/2016/10/20/pairwise-comparisons-of-proportions/</link>
      <pubDate>Thu, 20 Oct 2016 00:00:00 +0000</pubDate>
      
      <guid>/2016/10/20/pairwise-comparisons-of-proportions/</guid>
      <description>Pairwise comparison means comparing all pairs of something. If I have three items A, B and C, that means comparing A to B, A to C, and B to C. Given n items, I can determine the number of possible pairs using the binomial coefficient:
\[ \frac{n!}{2!(n - 2)!} = \binom {n}{2}\]
Using the R statistical computing environment, we can use the choose function to quickly calculate this. For example, how many possible 2-item combinations can I “choose” from 10 items:</description>
    </item>
    
    <item>
      <title>A tidyr Tutorial</title>
      <link>/2016/08/24/a-tidyr-tutorial/</link>
      <pubDate>Wed, 24 Aug 2016 00:00:00 +0000</pubDate>
      
      <guid>/2016/08/24/a-tidyr-tutorial/</guid>
      <description>The tidyr package by Hadley Wickham centers on two functions: gather and spread. If you have struggled to understand what exactly these two functions do, this tutorial is for you.
To begin we need to wrap our heads around the idea of “key-value pairs”. The help pages for gather and spread use this terminology to explain how they work. Without some intuition for key-value pairs, it can be difficult to truly understand how these functions work.</description>
    </item>
    
    <item>
      <title>Getting Started with Factor Analysis</title>
      <link>/2016/08/01/getting-started-with-factor-analysis/</link>
      <pubDate>Mon, 01 Aug 2016 00:00:00 +0000</pubDate>
      
      <guid>/2016/08/01/getting-started-with-factor-analysis/</guid>
      <description>Take a look at the following correlation matrix for Olympic decathlon data calculated from 280 scores from 1960 through 2004 (Johnson and Wichern, p. 499):
If we focus on the first row, 100m (100 meter dash), we see that it has fairly high correlations with LJ and 400m (long jump and 400 meter dash, 0.64 and 0.55) and somewhat lower correlations with all other events. Scoring high in the 100 meter dash seems to correlate with scoring high in the long jump and 400 meter run but gives no solid indication of performance in the other events.</description>
    </item>
    
    <item>
      <title>An Introduction to Loglinear Models</title>
      <link>/2016/07/12/an-introduction-to-loglinear-models/</link>
      <pubDate>Tue, 12 Jul 2016 00:00:00 +0000</pubDate>
      
      <guid>/2016/07/12/an-introduction-to-loglinear-models/</guid>
      <description>Loglinear models model cell counts in contingency tables. They’re a little different from other modeling methods in that they don’t distinguish between response and explanatory variables. All variables in a loglinear model are essentially “responses”.
To learn more about loglinear models, we’ll explore the following data from Agresti (1996, Table 6.3). It summarizes responses from a survey that asked high school seniors in a particular city whether they had ever used alcohol, cigarettes, or marijuana.</description>
    </item>
    
    <item>
      <title>Setting up Color Palettes in R</title>
      <link>/2016/06/10/setting-up-color-palettes-in-r/</link>
      <pubDate>Fri, 10 Jun 2016 00:00:00 +0000</pubDate>
      
      <guid>/2016/06/10/setting-up-color-palettes-in-r/</guid>
      <description>Plotting with color in R is kind of like painting a room in your house: you have to pick some colors. R has some default colors ready to go, but it’s only natural to want to play around and try some different combinations. In this post we’ll look at some ways you can define new color palettes for plotting in R.
To begin, let’s use the palette function to see what colors are currently available:</description>
    </item>
    
    <item>
      <title>Getting Started with Hurdle Models</title>
      <link>/2016/06/01/getting-started-with-hurdle-models/</link>
      <pubDate>Wed, 01 Jun 2016 00:00:00 +0000</pubDate>
      
      <guid>/2016/06/01/getting-started-with-hurdle-models/</guid>
      <description>Hurdle Models are a class of models for count data that help handle excess zeros and overdispersion. To motivate their use, let’s look at some data in R.
The following data come with the AER package. It is a sample of 4,406 individuals, aged 66 and over, who were covered by Medicare in 1988. One of the variables the data provide is number of physician office visits. Let’s say we wish to model the number of vists (a count) by some of the other variables in the data set.</description>
    </item>
    
    <item>
      <title>Getting started with Negative Binomial Regression Modeling</title>
      <link>/2016/05/05/getting-started-with-negative-binomial-regression-modeling/</link>
      <pubDate>Thu, 05 May 2016 00:00:00 +0000</pubDate>
      
      <guid>/2016/05/05/getting-started-with-negative-binomial-regression-modeling/</guid>
      <description>When it comes to modeling counts (ie, whole numbers greater than or equal to 0), we often start with Poisson regression. This is a generalized linear model where a response is assumed to have a Poisson distribution conditional on a weighted sum of predictors. For example, we might model the number of documented concussions to NFL quarterbacks as a function of snaps played and the total years experience of his offensive line.</description>
    </item>
    
    <item>
      <title>Visualizing the Effects of Logistic Regression</title>
      <link>/2016/04/22/visualizing-the-effects-of-logistic-regression/</link>
      <pubDate>Fri, 22 Apr 2016 00:00:00 +0000</pubDate>
      
      <guid>/2016/04/22/visualizing-the-effects-of-logistic-regression/</guid>
      <description>Logistic regression is a popular and effective way of modeling a binary response. For example, we might wonder what influences a person to volunteer, or not volunteer, for psychological research. Some do, some don’t. Are there independent variables that would help explain or distinguish between those who volunteer and those who don’t? Logistic regression gives us a mathematical model that we can we use to estimate the probability of someone volunteering given certain independent variables.</description>
    </item>
    
    <item>
      <title>Understanding 2-way Interactions</title>
      <link>/2016/03/25/understanding-2-way-interactions/</link>
      <pubDate>Fri, 25 Mar 2016 00:00:00 +0000</pubDate>
      
      <guid>/2016/03/25/understanding-2-way-interactions/</guid>
      <description>When doing linear modeling or ANOVA it’s useful to examine whether or not the effect of one variable depends on the level of one or more variables. If it does then we have what is called an “interaction”. This means variables combine or interact to affect the response. The simplest type of interaction is the interaction between two two-level categorical variables. Let’s say we have gender (male and female), treatment (yes or no), and a continuous response measure.</description>
    </item>
    
    <item>
      <title>Comparing Proportions with Relative Risk and Odds Ratios</title>
      <link>/2016/01/08/comparing-proportions-with-relative-risk-and-odds-ratios/</link>
      <pubDate>Fri, 08 Jan 2016 00:00:00 +0000</pubDate>
      
      <guid>/2016/01/08/comparing-proportions-with-relative-risk-and-odds-ratios/</guid>
      <description>The classic two-by-two table displays counts of what may be called “successes” and “failures” versus some two-level grouping variable, such as gender (male and female) or treatment (placebo and active drug). An example of one such table is given in the book An Introduction to Categorical Data Analysis (Agresti, 1996, p. 20). The table classifies Myocardial Infarction (yes/no) with Group (Placebo/Aspirin). The data were “taken from a report on the relationship between aspirin use and myocardial infarction (heart attacks) by the Physicians’ Health Study Research Group at Harvard Medical School.</description>
    </item>
    
    <item>
      <title>Is R-squared Useless?</title>
      <link>/2015/10/17/is-r-squared-useless/</link>
      <pubDate>Sat, 17 Oct 2015 00:00:00 +0000</pubDate>
      
      <guid>/2015/10/17/is-r-squared-useless/</guid>
      <description>On Thursday, October 16, 2015, a disbelieving student posted on Reddit My stats professor just went on a rant about how R-squared values are essentially useless, is there any truth to this? It attracted a fair amount of attention, at least compared to other posts about statistics on Reddit.
It turns out the student’s stats professor was Cosma Shalizi of Carnegie Mellon University. Shalizi provides free and open access to his class lecture materials so we can see what exactly he was “ranting” about.</description>
    </item>
    
    <item>
      <title>Fitting and Interpreting a Proportional Odds Model</title>
      <link>/2015/10/05/fitting-and-interpreting-a-proportional-odds-model/</link>
      <pubDate>Mon, 05 Oct 2015 00:00:00 +0000</pubDate>
      
      <guid>/2015/10/05/fitting-and-interpreting-a-proportional-odds-model/</guid>
      <description>Take a look at the following table. It is a cross tabulation of data taken from the 1991 General Social Survey that relates political party affiliation to political ideology. (Agresti, An Introduction to Categorical Data Analysis, 1996)
td { padding:0 15px 0 15px; text-align: center; } caption { font-size: 70% }   Political Ideology by Party Affiliation, from the 1991 General Social Survey       Very Liberal</description>
    </item>
    
    <item>
      <title>Getting Started with Quantile Regression</title>
      <link>/2015/09/20/getting-started-with-quantile-regression/</link>
      <pubDate>Sun, 20 Sep 2015 00:00:00 +0000</pubDate>
      
      <guid>/2015/09/20/getting-started-with-quantile-regression/</guid>
      <description>When we think of regression we usually think of linear regression, the tried and true method for estimating a mean of some variable conditional on the levels or values of independent variables. In other words, we’re pretty sure the mean of our variable of interest differs depending on other variables. For example the mean weight of 1st year UVa males is some unknown value. But we could in theory take a random sample and discover there is a relationship between weight and height.</description>
    </item>
    
    <item>
      <title>Simulating Endogeneity</title>
      <link>/2015/09/10/simulating-endogeneity/</link>
      <pubDate>Thu, 10 Sep 2015 00:00:00 +0000</pubDate>
      
      <guid>/2015/09/10/simulating-endogeneity/</guid>
      <description>First off, what is endogeneity, and why would we want to simulate it?
Endogeneity occurs when a statistical model has an independent variable that is correlated with the error term. The reason we would want to simulate it is to understand what exactly that definition means!
Let’s first simulate ideal data for simple linear regression using R.
# independent variable x &amp;lt;- seq(1,5,length.out = 100) # error (uncorrelated with x) set.</description>
    </item>
    
    <item>
      <title>Understanding Q-Q Plots</title>
      <link>/2015/08/26/understanding-q-q-plots/</link>
      <pubDate>Wed, 26 Aug 2015 00:00:00 +0000</pubDate>
      
      <guid>/2015/08/26/understanding-q-q-plots/</guid>
      <description>The Q-Q plot, or quantile-quantile plot, is a graphical tool to help us assess if a set of data plausibly came from some theoretical distribution such as a Normal or exponential. For example, if we run a statistical analysis that assumes our dependent variable is Normally distributed, we can use a Normal Q-Q plot to check that assumption. It’s just a visual check, not an air-tight proof, so it is somewhat subjective.</description>
    </item>
    
  </channel>
</rss>